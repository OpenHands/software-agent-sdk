---
# Run evaluation on a PR, after releases, or manually
name: Run Eval

# Runs when a PR is labeled with one of the "run-eval-" labels, after releases, or manually triggered
on:
    pull_request:
        types: [labeled]
    release:
        types: [published]
    workflow_dispatch:
        inputs:
            branch:
                description: Branch to evaluate
                required: true
                default: main
            eval_instances:
                description: Number of evaluation instances
                required: true
                default: '50'
                type: choice
                options:
                    - '1'
                    - '5'
                    - '50'
                    - '200'
            models:
                description: Comma-separated model configurations to evaluate (e.g. "claude_sonnet_4_5,haiku_4_5")
                required: true
                default: claude_sonnet_4_5
                type: string
            reason:
                description: Reason for manual trigger
                required: false
                default: ''

env:
    # Environment variable for the master GitHub issue number where all evaluation results will be commented
    # This should be set to the issue number where you want all evaluation results to be posted
    MASTER_EVAL_ISSUE_NUMBER: ${{ vars.MASTER_EVAL_ISSUE_NUMBER || '0' }}
    MODELS_JSON: >-
        [
          {"id":"claude_sonnet_4_5","display_name":"Claude Sonnet 4.5","llm_config":{"model":"litellm_proxy/claude-sonnet-4-5-20250929","temperature":0.0}},
          {"id":"haiku_4_5","display_name":"Claude Haiku 4.5","llm_config":{"model":"litellm_proxy/claude-haiku-4-5-20251001","temperature":0.0}},
          {"id":"gpt_5_mini","display_name":"GPT-5 Mini","llm_config":{"model":"litellm_proxy/gpt-5-mini-2025-08-07","temperature":1.0}},
          {"id":"deepseek_chat","display_name":"DeepSeek Chat","llm_config":{"model":"litellm_proxy/deepseek/deepseek-chat"}},
          {"id":"kimi_k2_thinking","display_name":"Kimi K2 Thinking","llm_config":{"model":"litellm_proxy/moonshot/kimi-k2-thinking"}}
        ]

jobs:
    trigger-eval:
        name: Trigger remote eval
        if: >
            (github.event_name == 'pull_request' &&
             startsWith(github.event.label.name, 'run-eval-')) ||
            github.event_name == 'release' ||
            github.event_name == 'workflow_dispatch'
        runs-on: blacksmith-4vcpu-ubuntu-2204

        steps:
            - name: Checkout branch
              uses: actions/checkout@v4
              with:
                  ref: ${{ github.event_name == 'pull_request' && github.head_ref || (github.event_name == 'workflow_dispatch' && 
                      github.event.inputs.branch) || github.ref }}

            - name: Set evaluation parameters
              id: eval_params
              env:
                  EVENT_NAME: ${{ github.event_name }}
                  EVENT_PATH: ${{ github.event_path }}
                  MASTER_ISSUE: ${{ env.MASTER_EVAL_ISSUE_NUMBER }}
                  REPO: ${{ github.repository }}
                  RUN_ID: ${{ github.run_id }}
                  MODELS_JSON: ${{ env.MODELS_JSON }}
              run: |
                  python <<'PY'
                  import json
                  import os
                  import re
                  import sys

                  def emit(key: str, value: str) -> None:
                      with open(os.environ["GITHUB_OUTPUT"], "a", encoding="utf-8") as fh:
                          fh.write(f"{key}={value}\n")

                  models = {m["id"]: m for m in json.loads(os.environ["MODELS_JSON"])}
                  event_name = os.environ["EVENT_NAME"]
                  master_issue = os.environ["MASTER_ISSUE"]
                  event = json.load(open(os.environ["EVENT_PATH"], "r", encoding="utf-8"))

                  targets: list[dict] = []
                  pr_number = master_issue
                  eval_branch = ""
                  eval_instances = "50"

                  def build_target(model_id: str, eval_instances: str) -> dict:
                      model = models[model_id]
                      return {
                          "model_id": model_id,
                          "llm_config": model["llm_config"],
                          "display_name": model["display_name"],
                          "eval_instances": eval_instances,
                      }

                  if event_name == "pull_request":
                      label = event.get("label", {}).get("name") or ""
                      match = re.match(r"^run-eval-([a-z0-9\-]+)-([0-9]+)$", label)
                      if not match:
                          emit("should_run", "false")
                          print(f"::warning::Label '{label}' does not match run-eval-<model>-<count> pattern; skipping eval trigger")
                          sys.exit(0)
                      model_id, eval_instances = match.groups()
                      if model_id not in models:
                          raise SystemExit(f"Unsupported model '{model_id}'")
                      targets = [build_target(model_id, eval_instances)]
                      eval_branch = event["pull_request"]["head"]["ref"]
                      pr_number = str(event["pull_request"]["number"])
                  elif event_name == "workflow_dispatch":
                      inputs = event.get("inputs", {})
                      raw_models = inputs.get("models") or "claude_sonnet_4_5"
                      eval_instances = inputs.get("eval_instances") or "50"
                      requested_models = [
                          m.strip() for m in raw_models.split(",") if m.strip()
                      ]
                      if not requested_models:
                          raise SystemExit("No valid models provided in 'models' input")
                      invalid = [m for m in requested_models if m not in models]
                      if invalid:
                          raise SystemExit(f"Unsupported model(s) {invalid!r}")
                      targets = [build_target(model_id, eval_instances) for model_id in requested_models]
                      eval_branch = inputs.get("branch") or "main"
                  elif event_name == "release":
                      eval_branch = str(event.get("release", {}).get("tag_name") or "")
                      if not eval_branch:
                          eval_branch = event.get("ref", "main")
                      eval_instances = "200"
                      targets = [
                          build_target(model_id, eval_instances)
                          for model_id in models
                      ]
                  else:
                      emit("should_run", "false")
                      print(f"::warning::Unsupported event '{event_name}' for this workflow")
                      sys.exit(0)

                  emit("should_run", "true")
                  repo_url = f"https://github.com/{os.environ['REPO']}"
                  emit("repo_url", repo_url)
                  emit("eval_branch", eval_branch)
                  emit("eval_instances", eval_instances)
                  emit("targets", json.dumps(targets))
                  emit("models_text", ", ".join(f"{t['display_name']} ({t['eval_instances']})" for t in targets))
                  PY

            - name: Trigger remote job
              if: steps.eval_params.outputs.should_run == 'true'
              env:
                  PAT_TOKEN: ${{ secrets.ALLHANDS_BOT_GITHUB_PAT }}
                  TARGETS: ${{ steps.eval_params.outputs.targets }}
                  EVAL_BRANCH: ${{ steps.eval_params.outputs.eval_branch }}
                  MODELS_TEXT: ${{ steps.eval_params.outputs.models_text }}
                  EVAL_INSTANCES: ${{ steps.eval_params.outputs.eval_instances }}
                  REPO_URL: ${{ steps.eval_params.outputs.repo_url }}
              run: |
                  if [ -z "$PAT_TOKEN" ]; then
                    echo "PAT_TOKEN is required to dispatch remote workflow"
                    exit 1
                  fi

                  echo "$TARGETS" | jq -c '.[]' | while read -r target; do
                    model_id=$(echo "$target" | jq -r '.model_id')
                    llm_config=$(echo "$target" | jq -c '.llm_config')
                    eval_instances=$(echo "$target" | jq -r '.eval_instances')

                    payload=$(jq -n \
                      --arg repo "$REPO_URL" \
                      --arg sdk_ref "$EVAL_BRANCH" \
                      --arg pr "$PR_NUMBER" \
                      --arg instances "$eval_instances" \
                      --arg model_id "$model_id" \
                      --arg llm_config "$llm_config" \
                      '{
                        "ref": "main",
                        "inputs": {
                          "sdk-repo": $repo,
                          "sdk-ref": $sdk_ref,
                          "benchmarks-repo": "https://github.com/OpenHands/benchmarks.git",
                          "benchmarks-ref": "main",
                          "pr-number": $pr,
                          "eval-instances": $instances,
                          "llm-model-id": $model_id,
                          "llm-config": $llm_config,
                          "trigger-description": "",
                          "trigger_type": "manual"
                        }
                      }')

                    curl -sS -X POST \
                      -H "Authorization: Bearer $PAT_TOKEN" \
                      -H "Accept: application/vnd.github+json" \
                      -d "$payload" \
                      https://api.github.com/repos/OpenHands/evaluation/actions/workflows/create-sdk-eval.yml/dispatches
                  done

                  if [ -n "${{ secrets.SLACK_TOKEN }}" ]; then
                    if [[ "${{ github.event_name }}" == "pull_request" ]]; then
                      TRIGGER_URL="https://github.com/${{ github.repository }}/pull/${{ github.event.pull_request.number }}"
                      slack_text="PR $TRIGGER_URL has triggered evaluation on $EVAL_INSTANCES instances with models: $MODELS_TEXT (branch $EVAL_BRANCH)."
                    elif [[ "${{ github.event_name }}" == "release" ]]; then
                      TRIGGER_URL="https://github.com/${{ github.repository }}/releases/tag/${{ github.ref_name }}"
                      slack_text="Release $TRIGGER_URL has triggered evaluation on $EVAL_INSTANCES instances with models: $MODELS_TEXT (branch $EVAL_BRANCH)."
                    else
                      reason="${{ github.event.inputs.reason || 'No reason provided' }}"
                      TRIGGER_URL="https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}"
                      slack_text="Manual trigger ($reason) has triggered evaluation on $EVAL_INSTANCES instances with models: $MODELS_TEXT for branch $EVAL_BRANCH."
                    fi
                    curl -sS -X POST -H 'Content-type: application/json' --data "{\"text\":\"$slack_text\"}" \
                      https://hooks.slack.com/services/${{ secrets.SLACK_TOKEN }}
                  fi

            - name: Comment on issue/PR
              if: steps.eval_params.outputs.should_run == 'true'
              uses: KeisukeYamashita/create-comment@v1
              with:
                  number: ${{ github.event_name == 'pull_request' && github.event.pull_request.number || env.MASTER_EVAL_ISSUE_NUMBER }}
                  unique: false
                  comment: |
                      **Evaluation Triggered**

                      **Trigger:** ${{ github.event_name == 'pull_request' && format('Pull Request #{0}', github.event.pull_request.number) || (github.event_name == 'release' && 'Release') || format('Manual Trigger: {0}', github.event.inputs.reason || 'No reason provided') }}
                      **Branch:** ${{ steps.eval_params.outputs.eval_branch }}
                      **Instances:** ${{ steps.eval_params.outputs.eval_instances }}
                      **Models:** ${{ steps.eval_params.outputs.models_text }}
                      **Commit:** ${{ github.sha }}

                      Running evaluation on the specified branch. Once eval is done, the results will be posted here.
