---
# Run evaluation on a PR, after releases, or manually
name: Run Eval

# Runs when a PR is labeled with one of the "run-eval-" labels, after releases, or manually triggered
on:
    pull_request:
        types: [labeled]
    release:
        types: [published]
    workflow_dispatch:
        inputs:
            branch:
                description: Branch to evaluate
                required: true
                default: main
            eval_instances:
                description: Number of evaluation instances
                required: true
                default: '50'
                type: choice
                options:
                    - '1'
                    - '5'
                    - '50'
                    - '200'
            models:
                description: Comma-separated model configurations to evaluate (e.g. "claude_sonnet_4_5,haiku_4_5")
                required: true
                default: claude_sonnet_4_5
                type: string
            reason:
                description: Reason for manual trigger
                required: false
                default: ''

env:
    # Environment variable for the master GitHub issue number where all evaluation results will be commented
    # This should be set to the issue number where you want all evaluation results to be posted
    MASTER_EVAL_ISSUE_NUMBER: ${{ vars.MASTER_EVAL_ISSUE_NUMBER || '0' }}
    MODELS_JSON: >-
        [
          {"id":"claude_sonnet_4_5","display_name":"Claude Sonnet 4.5","llm_config":{"model":"litellm_proxy/claude-sonnet-4-5-20250929","temperature":0.0}},
          {"id":"haiku_4_5","display_name":"Claude Haiku 4.5","llm_config":{"model":"litellm_proxy/claude-haiku-4-5-20251001","temperature":0.0}},
          {"id":"gpt_5_mini","display_name":"GPT-5 Mini","llm_config":{"model":"litellm_proxy/gpt-5-mini-2025-08-07","temperature":1.0}},
          {"id":"deepseek_chat","display_name":"DeepSeek Chat","llm_config":{"model":"litellm_proxy/deepseek/deepseek-chat"}},
          {"id":"kimi_k2_thinking","display_name":"Kimi K2 Thinking","llm_config":{"model":"litellm_proxy/moonshot/kimi-k2-thinking"}}
        ]

jobs:
    trigger-eval:
        name: Trigger remote eval
        if: >
            (github.event_name == 'pull_request' &&
             startsWith(github.event.label.name, 'run-eval-')) ||
            github.event_name == 'release' ||
            github.event_name == 'workflow_dispatch'
        runs-on: blacksmith-4vcpu-ubuntu-2204

        steps:
            - name: Checkout repository
              uses: actions/checkout@v4
              with:
                  ref: ${{ github.event_name == 'pull_request' && github.head_ref || (github.event_name == 'workflow_dispatch' && 
                      github.event.inputs.branch) || github.ref }}

            - name: Determine evaluation parameters
              id: params
              env:
                  EVENT_NAME: ${{ github.event_name }}
                  EVENT_PATH: ${{ github.event_path }}
                  MASTER_ISSUE: ${{ env.MASTER_EVAL_ISSUE_NUMBER }}
                  REPO: ${{ github.repository }}
                  RUN_ID: ${{ github.run_id }}
                  MODELS_JSON: ${{ env.MODELS_JSON }}
              run: |
                  python <<'PY'
                  import json
                  import os
                  import re
                  import sys

                  def emit(key: str, value: str) -> None:
                      with open(os.environ["GITHUB_OUTPUT"], "a", encoding="utf-8") as fh:
                          fh.write(f"{key}={value}\n")

                  models = {m["id"]: m for m in json.loads(os.environ["MODELS_JSON"])}
                  event_name = os.environ["EVENT_NAME"]
                  master_issue = os.environ["MASTER_ISSUE"]
                  event = json.load(open(os.environ["EVENT_PATH"], "r", encoding="utf-8"))

                  targets: list[dict] = []
                  trigger_desc = ""
                  trigger_url = ""
                  trigger_type = ""
                  pr_number = master_issue
                  sdk_ref = ""

                  def build_target(model_id: str, eval_instances: str) -> dict:
                      model = models[model_id]
                      return {
                          "model_id": model_id,
                          "llm_config": model["llm_config"],
                          "display_name": model["display_name"],
                          "eval_instances": eval_instances,
                      }

                  if event_name == "pull_request":
                      label = event.get("label", {}).get("name") or ""
                      match = re.match(r"^run-eval-([a-z0-9\-]+)-([0-9]+)$", label)
                      if not match:
                          emit("should_run", "false")
                          print(f"::warning::Label '{label}' does not match run-eval-<model>-<count> pattern; skipping eval trigger")
                          sys.exit(0)
                      model_id, eval_instances = match.groups()
                      if model_id not in models:
                          raise SystemExit(f"Unsupported model '{model_id}'")
                      targets = [build_target(model_id, eval_instances)]
                      sdk_ref = event["pull_request"]["head"]["ref"]
                      pr_number = str(event["pull_request"]["number"])
                      trigger_desc = f"Pull Request #{pr_number}"
                      trigger_url = event["pull_request"].get("html_url") or ""
                      trigger_type = "pull_request_label"
                  elif event_name == "workflow_dispatch":
                      inputs = event.get("inputs", {})
                      raw_models = inputs.get("models") or "claude_sonnet_4_5"
                      eval_instances = inputs.get("eval_instances") or "50"
                      requested_models = [
                          m.strip() for m in raw_models.split(",") if m.strip()
                      ]
                      if not requested_models:
                          raise SystemExit("No valid models provided in 'models' input")
                      invalid = [m for m in requested_models if m not in models]
                      if invalid:
                          raise SystemExit(f"Unsupported model(s) {invalid!r}")
                      targets = [build_target(model_id, eval_instances) for model_id in requested_models]
                      sdk_ref = inputs.get("branch") or "main"
                      reason = inputs.get("reason") or "Manual trigger"
                      trigger_desc = f"Manual trigger ({reason})"
                      trigger_url = f"https://github.com/{os.environ['REPO']}/actions/runs/{os.environ['RUN_ID']}"
                      trigger_type = "manual"
                  elif event_name == "release":
                      sdk_ref = str(event.get("release", {}).get("tag_name") or "")
                      if not sdk_ref:
                          sdk_ref = event.get("ref", "main")
                      targets = [
                          build_target(model_id, "200")
                          for model_id in models
                      ]
                      trigger_desc = f"Release {sdk_ref}"
                      trigger_url = event.get("release", {}).get("html_url") or ""
                      trigger_type = "release"
                  else:
                      emit("should_run", "false")
                      print(f"::warning::Unsupported event '{event_name}' for this workflow")
                      sys.exit(0)

                  emit("should_run", "true")
                  emit("sdk_ref", sdk_ref)
                  emit("pr_number", pr_number)
                  emit("targets", json.dumps(targets))
                  emit("trigger_desc", trigger_desc)
                  emit("trigger_url", trigger_url)
                  emit("trigger_type", trigger_type)
                  emit("models_text", ", ".join(f"{t['display_name']} ({t['eval_instances']})" for t in targets))
                  PY

            - name: Trigger evaluation workflow(s)
              if: steps.params.outputs.should_run == 'true'
              env:
                  PAT_TOKEN: ${{ secrets.ALLHANDS_BOT_GITHUB_PAT }}
                  SDK_REPO: https://github.com/${{ github.repository }}
                  SDK_REF: ${{ steps.params.outputs.sdk_ref }}
                  PR_NUMBER: ${{ steps.params.outputs.pr_number }}
                  TARGETS: ${{ steps.params.outputs.targets }}
                  TRIGGER_DESC: ${{ steps.params.outputs.trigger_desc }}
                  TRIGGER_TYPE: ${{ steps.params.outputs.trigger_type }}
              run: |
                  if [ -z "$PAT_TOKEN" ]; then
                    echo "PAT_TOKEN is required to dispatch remote workflow"
                    exit 1
                  fi

                  echo "$TARGETS" | jq -c '.[]' | while read -r target; do
                    model_id=$(echo "$target" | jq -r '.model_id')
                    llm_config=$(echo "$target" | jq -c '.llm_config')
                    eval_instances=$(echo "$target" | jq -r '.eval_instances')

                    payload=$(jq -n \
                      --arg repo "$SDK_REPO" \
                      --arg sdk_ref "$SDK_REF" \
                      --arg pr "$PR_NUMBER" \
                      --arg instances "$eval_instances" \
                      --arg model_id "$model_id" \
                      --arg llm_config "$llm_config" \
                      --arg trigger_desc "$TRIGGER_DESC" \
                      --arg trigger_type "$TRIGGER_TYPE" \
                      '{
                        "ref": "main",
                        "inputs": {
                          "sdk-repo": $repo,
                          "sdk-ref": $sdk_ref,
                          "benchmarks-repo": "https://github.com/OpenHands/benchmarks.git",
                          "benchmarks-ref": "main",
                          "pr-number": $pr,
                          "eval-instances": $instances,
                          "llm-model-id": $model_id,
                          "llm-config": $llm_config,
                          "trigger-description": $trigger_desc,
                          "trigger_type": $trigger_type
                        }
                      }')

                    curl -sS -X POST \
                      -H "Authorization: Bearer $PAT_TOKEN" \
                      -H "Accept: application/vnd.github+json" \
                      -d "$payload" \
                      https://api.github.com/repos/OpenHands/evaluation/actions/workflows/create-sdk-eval.yml/dispatches
                  done

            - name: Notify Slack
              if: steps.params.outputs.should_run == 'true'
              env:
                  SLACK_TOKEN: ${{ secrets.SLACK_TOKEN }}
                  TRIGGER_DESC: ${{ steps.params.outputs.trigger_desc }}
                  TRIGGER_URL: ${{ steps.params.outputs.trigger_url }}
                  SDK_REF: ${{ steps.params.outputs.sdk_ref }}
                  MODELS_TEXT: ${{ steps.params.outputs.models_text }}
              run: |
                  if [ -z "$SLACK_TOKEN" ]; then
                    echo "SLACK_TOKEN secret not configured; skipping Slack notification"
                    exit 0
                  fi
                  link="$TRIGGER_DESC"
                  if [ -n "$TRIGGER_URL" ]; then
                    link="$TRIGGER_DESC ($TRIGGER_URL)"
                  fi
                  text="SDK eval triggered for $link on ref $SDK_REF with models: $MODELS_TEXT."
                  curl -sS -X POST -H 'Content-type: application/json' \
                    --data "{\"text\":\"$text\"}" \
                    https://hooks.slack.com/services/$SLACK_TOKEN

            - name: Comment on PR / issue
              if: steps.params.outputs.should_run == 'true'
              uses: KeisukeYamashita/create-comment@v1
              with:
                  number: ${{ steps.params.outputs.pr_number }}
                  unique: false
                  comment: |
                      **SDK Evaluation Triggered**

                      **Trigger:** ${{ steps.params.outputs.trigger_desc }}
                      **Branch/Tag:** `${{ steps.params.outputs.sdk_ref }}`
                      **Models:** ${{ steps.params.outputs.models_text }}

                      Results will be posted here once the eval job completes.
